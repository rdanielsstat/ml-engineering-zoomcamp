{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00775a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.3\n",
      "Numpy version: 2.3.4\n",
      "Scikit-learn version: 1.7.2\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"Pandas version: \" + str(pd.__version__))\n",
    "print(\"Numpy version: \" + str(np.__version__))\n",
    "print(\"Scikit-learn version: \" + str(version(\"scikit-learn\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4a703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed test data\n",
    "with open('../data/temp/target_encoded_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_test_encoded = data['X_test']\n",
    "X_test_orig = data['X_test_orig']\n",
    "\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d349eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  2.300 days\n",
      "RMSE: 5.771 days\n",
      "R2:   0.125\n"
     ]
    }
   ],
   "source": [
    "# load ols model\n",
    "with open('../models/ols_model.pkl', 'rb') as f:\n",
    "    ols_saved = pickle.load(f)\n",
    "\n",
    "# evaluate OLS model on test data\n",
    "y_test_orig = np.exp(y_test)\n",
    "\n",
    "# get features used by OLS\n",
    "ols_features = ols_saved['feature_names']\n",
    "X_test_ols = X_test_encoded[ols_features]\n",
    "\n",
    "# standardize\n",
    "X_test_ols_scaled = ols_saved['scaler'].transform(X_test_ols)\n",
    "\n",
    "# Convert back to DataFrame to preserve feature names\n",
    "X_test_ols_scaled = pd.DataFrame(\n",
    "    X_test_ols_scaled, \n",
    "    columns = ols_features,\n",
    "    index = X_test_ols.index\n",
    ")\n",
    "\n",
    "# predict (on log scale)\n",
    "y_test_pred_ols_log = ols_saved['ols_model'].predict(X_test_ols_scaled)\n",
    "\n",
    "# transform to original scale\n",
    "y_test_pred_ols = np.exp(y_test_pred_ols_log)\n",
    "\n",
    "# evaluate\n",
    "mae_ols = mean_absolute_error(y_test_orig, y_test_pred_ols)\n",
    "rmse_ols = np.sqrt(mean_squared_error(y_test_orig, y_test_pred_ols))\n",
    "r2_ols = r2_score(y_test_orig, y_test_pred_ols)\n",
    "\n",
    "print(f\"MAE:  {mae_ols:.3f} days\")\n",
    "print(f\"RMSE: {rmse_ols:.3f} days\")\n",
    "print(f\"R2:   {r2_ols:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587b35d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  2.397 days\n",
      "RMSE: 5.843 days\n",
      "R2:   0.103\n"
     ]
    }
   ],
   "source": [
    "with open('../models/rf_model.pkl', 'rb') as f:\n",
    "    rf_saved = pickle.load(f)\n",
    "\n",
    "# predict (on log scale)\n",
    "y_test_pred_rf = rf_saved['rf_model'].predict(X_test_encoded)\n",
    "\n",
    "# transform to original scale\n",
    "# y_test_pred_rf = np.exp(y_test_pred_rf_log)\n",
    "\n",
    "# evaluate\n",
    "mae_rf = mean_absolute_error(y_test_orig, y_test_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test_orig, y_test_pred_rf))\n",
    "r2_rf = r2_score(y_test_orig, y_test_pred_rf)\n",
    "\n",
    "print(f\"MAE:  {mae_rf:.3f} days\")\n",
    "print(f\"RMSE: {rmse_rf:.3f} days\")\n",
    "print(f\"R2:   {r2_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5842f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  2.371 days\n",
      "RMSE: 5.839 days\n",
      "R2:   0.105\n"
     ]
    }
   ],
   "source": [
    "with open('../models/catboost_model.pkl', 'rb') as f:\n",
    "    catboost_saved = pickle.load(f)\n",
    "\n",
    "# predict (already on original scale)\n",
    "y_test_pred_cat = catboost_saved['catboost_model'].predict(X_test_orig)\n",
    "\n",
    "# evaluate\n",
    "mae_cat = mean_absolute_error(y_test_orig, y_test_pred_cat)\n",
    "rmse_cat = np.sqrt(mean_squared_error(y_test_orig, y_test_pred_cat))\n",
    "r2_cat = r2_score(y_test_orig, y_test_pred_cat)\n",
    "\n",
    "print(f\"MAE:  {mae_cat:.3f} days\")\n",
    "print(f\"RMSE: {rmse_cat:.3f} days\")\n",
    "print(f\"R2:   {r2_cat:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8dfe5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9303d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4fbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7199bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88f0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f428deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate why test r2 is so low\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "y_test_orig = np.exp(y_test)\n",
    "\n",
    "# Select features\n",
    "X_test_reduced = X_test[retained_features]\n",
    "\n",
    "# Use TRANSFORM not FIT_TRANSFORM\n",
    "X_test_scaled = scaler.transform(X_test_reduced)  # ← FIXED\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=retained_features, index=X_test_reduced.index)\n",
    "\n",
    "# Predict (log scale)\n",
    "y_test_pred_log_std = ols_standardized.predict(X_test_scaled)\n",
    "\n",
    "# Transform to original scale\n",
    "y_test_pred_std = np.exp(y_test_pred_log_std)\n",
    "\n",
    "# Evaluate with y_test_orig, not y_val_orig!\n",
    "rmse_std = np.sqrt(mean_squared_error(y_test_orig, y_test_pred_std))  # ← FIXED\n",
    "mae_std = mean_absolute_error(y_test_orig, y_test_pred_std)  # ← FIXED\n",
    "r2_std = r2_score(y_test_orig, y_test_pred_std)  # ← FIXED\n",
    "\n",
    "print(\"\\nOLS with standardized features (evaluated on original scale):\")\n",
    "print(f\"RMSE: {rmse_std:.3f}, MAE: {mae_std:.3f}, R²: {r2_std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compare distributions\n",
    "print(\"=\"*80)\n",
    "print(\"TARGET VARIABLE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get original scale targets\n",
    "y_train_orig = np.exp(y_train)\n",
    "y_val_orig = np.exp(y_val)\n",
    "y_test_orig = np.exp(y_test)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(f\"{'Metric':<20} {'Train':<15} {'Validation':<15} {'Test':<15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Mean':<20} {y_train_orig.mean():<15.2f} {y_val_orig.mean():<15.2f} {y_test_orig.mean():<15.2f}\")\n",
    "print(f\"{'Std Dev':<20} {y_train_orig.std():<15.2f} {y_val_orig.std():<15.2f} {y_test_orig.std():<15.2f}\")\n",
    "print(f\"{'Variance':<20} {y_train_orig.var():<15.2f} {y_val_orig.var():<15.2f} {y_test_orig.var():<15.2f}\")\n",
    "print(f\"{'Min':<20} {y_train_orig.min():<15.2f} {y_val_orig.min():<15.2f} {y_test_orig.min():<15.2f}\")\n",
    "print(f\"{'Max':<20} {y_train_orig.max():<15.2f} {y_val_orig.max():<15.2f} {y_test_orig.max():<15.2f}\")\n",
    "print(f\"{'Median':<20} {y_train_orig.median():<15.2f} {y_val_orig.median():<15.2f} {y_test_orig.median():<15.2f}\")\n",
    "print(f\"{'Q1':<20} {y_train_orig.quantile(0.25):<15.2f} {y_val_orig.quantile(0.25):<15.2f} {y_test_orig.quantile(0.25):<15.2f}\")\n",
    "print(f\"{'Q3':<20} {y_train_orig.quantile(0.75):<15.2f} {y_val_orig.quantile(0.75):<15.2f} {y_test_orig.quantile(0.75):<15.2f}\")\n",
    "\n",
    "# Key insight: Check if test has more variance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE RATIO (higher = more variable)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test variance / Val variance: {y_test_orig.var() / y_val_orig.var():.2f}x\")\n",
    "print(f\"Test variance / Train variance: {y_test_orig.var() / y_train_orig.var():.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff488012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histograms\n",
    "axes[0, 0].hist(y_train_orig, bins=50, alpha=0.5, label='Train', edgecolor='black')\n",
    "axes[0, 0].hist(y_val_orig, bins=50, alpha=0.5, label='Validation', edgecolor='black')\n",
    "axes[0, 0].hist(y_test_orig, bins=50, alpha=0.5, label='Test', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Length of Stay (days)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution Comparison')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Box plots\n",
    "axes[0, 1].boxplot([y_train_orig, y_val_orig, y_test_orig], \n",
    "                    labels=['Train', 'Validation', 'Test'])\n",
    "axes[0, 1].set_ylabel('Length of Stay (days)')\n",
    "axes[0, 1].set_title('Box Plot Comparison')\n",
    "\n",
    "# Log scale histograms\n",
    "axes[1, 0].hist(y_train, bins=50, alpha=0.5, label='Train', edgecolor='black')\n",
    "axes[1, 0].hist(y_val, bins=50, alpha=0.5, label='Validation', edgecolor='black')\n",
    "axes[1, 0].hist(y_test, bins=50, alpha=0.5, label='Test', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Log(Length of Stay)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution Comparison (Log Scale)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Cumulative distributions\n",
    "axes[1, 1].hist(y_train_orig, bins=100, cumulative=True, density=True, \n",
    "                histtype='step', linewidth=2, label='Train')\n",
    "axes[1, 1].hist(y_val_orig, bins=100, cumulative=True, density=True, \n",
    "                histtype='step', linewidth=2, label='Validation')\n",
    "axes[1, 1].hist(y_test_orig, bins=100, cumulative=True, density=True, \n",
    "                histtype='step', linewidth=2, label='Test')\n",
    "axes[1, 1].set_xlabel('Length of Stay (days)')\n",
    "axes[1, 1].set_ylabel('Cumulative Probability')\n",
    "axes[1, 1].set_title('Cumulative Distribution')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count extreme values\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTREME VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "thresholds = [10, 20, 30, 50, 100]\n",
    "for threshold in thresholds:\n",
    "    train_count = (y_train_orig > threshold).sum()\n",
    "    val_count = (y_val_orig > threshold).sum()\n",
    "    test_count = (y_test_orig > threshold).sum()\n",
    "    \n",
    "    train_pct = train_count / len(y_train_orig) * 100\n",
    "    val_pct = val_count / len(y_val_orig) * 100\n",
    "    test_pct = test_count / len(y_test_orig) * 100\n",
    "    \n",
    "    print(f\"\\nLoS > {threshold} days:\")\n",
    "    print(f\"  Train: {train_count} ({train_pct:.1f}%)\")\n",
    "    print(f\"  Val:   {val_count} ({val_pct:.1f}%)\")\n",
    "    print(f\"  Test:  {test_count} ({test_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand R² breakdown\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"R² BREAKDOWN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def r2_components(y_true, y_pred):\n",
    "    ss_res = ((y_true - y_pred) ** 2).sum()  # Residual sum of squares\n",
    "    ss_tot = ((y_true - y_true.mean()) ** 2).sum()  # Total sum of squares\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return ss_res, ss_tot, r2\n",
    "\n",
    "# Validation\n",
    "y_val_pred = np.exp(ols_standardized.predict(X_val_scaled))\n",
    "ss_res_val, ss_tot_val, r2_val = r2_components(y_val_orig, y_val_pred)\n",
    "\n",
    "# Test\n",
    "y_test_pred = np.exp(y_test_pred_log_std)\n",
    "ss_res_test, ss_tot_test, r2_test = r2_components(y_test_orig, y_test_pred)\n",
    "\n",
    "print(f\"{'Metric':<25} {'Validation':<20} {'Test':<20}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'SS_residual':<25} {ss_res_val:<20.2f} {ss_res_test:<20.2f}\")\n",
    "print(f\"{'SS_total':<25} {ss_tot_val:<20.2f} {ss_tot_test:<20.2f}\")\n",
    "print(f\"{'R²':<25} {r2_val:<20.3f} {r2_test:<20.3f}\")\n",
    "print(f\"\\nSS_total ratio (test/val): {ss_tot_test / ss_tot_val:.2f}x\")\n",
    "print(f\"SS_residual ratio (test/val): {ss_res_test / ss_res_val:.2f}x\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if ss_tot_test / ss_tot_val > 1.5:\n",
    "    print(\"  ⚠️  Test set has MUCH higher total variance\")\n",
    "    print(\"     This means test data is more spread out, making it harder to explain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prediction quality\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION QUALITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Correlation between predictions and actuals\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr_val, _ = pearsonr(y_val_orig, y_val_pred)\n",
    "corr_test, _ = pearsonr(y_test_orig, y_test_pred)\n",
    "\n",
    "print(f\"Correlation (predictions vs actuals):\")\n",
    "print(f\"  Validation: {corr_val:.3f}\")\n",
    "print(f\"  Test: {corr_test:.3f}\")\n",
    "\n",
    "# Scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(y_val_orig, y_val_pred, alpha=0.3)\n",
    "axes[0].plot([0, y_val_orig.max()], [0, y_val_orig.max()], 'r--', label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual LoS (days)')\n",
    "axes[0].set_ylabel('Predicted LoS (days)')\n",
    "axes[0].set_title(f'Validation Set (R²={r2_val:.3f})')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(y_test_orig, y_test_pred, alpha=0.3)\n",
    "axes[1].plot([0, y_test_orig.max()], [0, y_test_orig.max()], 'r--', label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual LoS (days)')\n",
    "axes[1].set_ylabel('Predicted LoS (days)')\n",
    "axes[1].set_title(f'Test Set (R²={r2_test:.3f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_orig = data['X_val_orig']\n",
    "X_test_orig = data['X_test_orig']\n",
    "\n",
    "# Compare key features between validation and test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE DISTRIBUTION: VALIDATION vs TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "key_features = retained_features\n",
    "\n",
    "for feature in key_features:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    \n",
    "    val_dist = X_val_orig[feature].value_counts(normalize=True).sort_index()\n",
    "    test_dist = X_test_orig[feature].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Validation %': val_dist * 100,\n",
    "        'Test %': test_dist * 100,\n",
    "        'Difference': (test_dist - val_dist) * 100\n",
    "    }).fillna(0)\n",
    "    \n",
    "    print(comparison.round(1))\n",
    "    \n",
    "    # Statistical test\n",
    "    from scipy.stats import chi2_contingency\n",
    "\n",
    "    # Build contingency table\n",
    "    val_counts = X_val_orig[feature].value_counts().reindex(common, fill_value=0)\n",
    "    test_counts = X_test_orig[feature].value_counts().reindex(common, fill_value=0)\n",
    "\n",
    "    # Remove categories where BOTH sets have 0\n",
    "    mask = ~((val_counts == 0) & (test_counts == 0))\n",
    "    val_counts = val_counts[mask]\n",
    "    test_counts = test_counts[mask]\n",
    "\n",
    "    # If fewer than 2 categories remain, skip test\n",
    "    if len(val_counts) < 2:\n",
    "        print(\"  ⚠️ Not enough non-zero categories for chi-square test\")\n",
    "        continue\n",
    "\n",
    "    contingency = np.vstack([val_counts.values, test_counts.values])\n",
    "\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "    if p < 0.05:\n",
    "        print(f\"  ⚠️  SIGNIFICANTLY DIFFERENT (p={p:.4f})\")\n",
    "    else:\n",
    "        print(f\"  ✓ Similar distributions (p={p:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
